# -*- coding: utf-8 -*-
"""Bivariate Inverted Topp Leone thesis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lKHOk9NMINkxkYWGcq72EzwT6fP-xfgm

# Density Bivariate inverted topp Leone distribution
"""

import numpy as np
from scipy.special import gamma

def c_ibtl(nu1, nu2, xi):
    """
    Constante de normalización C(nu1, nu2, xi)
    """
    numerator = 2 * gamma(nu1 + nu2 + 1)
    denominator = (np.power(xi,nu1 + nu2)) * gamma(nu1) * gamma(nu2)
    return numerator / denominator

def pdf_ibtl(x, y, nu1, nu2, xi):
    """
    Densidad de la distribución bivariada invertida Topp-Leone IBTL(nu1, nu2, xi).

    Parámetros:
    -----------
    x : float o np.ndarray
        Valores positivos de la variable X (> 0)
    y : float o np.ndarray
        Valores positivos de la variable Y (> 0)
    nu1 : float > 0
        Parámetro de forma de X
    nu2 : float > 0
        Parámetro de forma de Y
    xi : float > 0
        Parámetro de escala compartido

    Retorna:
    --------
    float o np.ndarray
        Valor(es) de la densidad f(x, y)
    """
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=float)
    mask = (x > 0) & (y > 0)

    z = x + y
    coef = c_ibtl(nu1, nu2, xi)

    term1 = np.power(x,nu1 - 1)
    term2 = np.power(y,nu2 - 1)
    term3 = np.power(1 + z / xi,-2 * (nu1 + nu2) - 1)
    term4 = np.power(2 + z / xi, nu1 + nu2 - 1)

    pdf = np.zeros_like(x)
    pdf[mask] = coef * term1[mask] * term2[mask] * term3[mask] * term4[mask]
    return pdf

"""# Correlation"""

import numpy as np
from scipy.special import gamma, comb


def h2(nu, max_j=50):
    """
    Calcula h₂(ν) con suma truncada en j = max_j.

    h₂(ν) = sum_{j=0}^∞ sum_{i=0}^{j+2} (j+2 choose i) * (-1)^i *
            Γ[(1+i)/2] * Γ(ν) / Γ[ν + (1+i)/2]

    Parámetros:
    -----------
    nu : float > 0
        Parámetro ν
    max_j : int
        Límite superior de la suma (truncamiento)

    Retorna:
    --------
    float
        Aproximación de h₂(ν)
    """
    total = 0.0
    for j in range(max_j + 1):
        for i in range(j + 3):  # j+2 inclusive
            coef = comb(j + 2, i)
            sign = np.power(-1,i)
            gamma_num = gamma((1 + i) / 2) * gamma(nu)
            gamma_den = gamma(nu + (1 + i) / 2)
            term = coef * sign * gamma_num / gamma_den
            total += term
    return total

def rho_ibtl(nu1, nu2, xi):
    """
    Calcula la correlación ρ_{XY} para la distribución IBTL(nu1, nu2; xi).
    """

    nu = nu1 + nu2
    gamma_term = (np.sqrt(np.pi) * gamma(nu + 1)) / gamma(nu + 0.5)
    common_expr = np.power(gamma_term - 1,2)
    xi2 = np.power(xi,2)
    nu2_square = np.power(nu,2)

    # Varianza de X
    var_x = (
        (nu1 * (nu1 + 1)) / nu * xi2 * h2(nu)
        - (xi2 * nu1**2 / nu2_square) * common_expr
    )

    # Varianza de Y
    var_y = (
        (nu2 * (nu2 + 1)) / nu * xi2 * h2(nu)
        - (xi2 * np.power(nu2,2) / nu2_square) * common_expr
    )

    # Covarianza
    cov_xy = (
        ((nu1 * nu2) / nu) * xi2 * h2(nu)
        - (xi2 * nu1 * nu2 / nu2_square) * common_expr
    )

    # Correlación
    rho_xy = cov_xy / np.sqrt(var_x * var_y)

    return rho_xy, cov_xy, var_x, var_y

# prompt: haz un codigo python que calcule la tabla de correlaciones entre X1 y X2 calculada con la funcion  rho_ibtl y resultado almacenado en rho_xy para  valores de nu2= 0.5  ,1   ,2     ,3  ,5 , 10   , 20 y valores de nu1 =   0.10
#  0.25
# 0.50
# 1.00
# 2.00
#  3.00 &
# 5.00
# 10.00
#  15.00
# 20.00
#  25.00

import numpy as np
import pandas as pd

# Define los valores de nu1 y nu2
nu1_values = np.array([0.10, 0.25, 0.50, 1.00, 2.00, 3.00, 5.00, 10.00, 15.00, 20.00, 25.00])
nu2_values = np.array([0.5, 1.0, 2.0, 3.0, 5.0, 10.0, 20.0])
xi_value = 1.0  # Asumimos un valor para xi, puedes cambiarlo si es necesario

# Crea un DataFrame para almacenar los resultados
rho_df = pd.DataFrame(index=nu1_values, columns=nu2_values)

# Calcula la correlación para cada combinación de nu1 y nu2
for nu1 in nu1_values:
    for nu2 in nu2_values:
        # Verifica que nu1 y nu2 sean mayores que 0
        if nu1 > 0 and nu2 > 0:
            try:
                rho_xy, cov_xy, var_x, var_y = rho_ibtl(nu1, nu2, xi_value)
                rho_df.loc[nu1, nu2] = rho_xy
            except Exception as e:
                print(f"Error calculating rho for nu1={nu1}, nu2={nu2}: {e}")
                rho_df.loc[nu1, nu2] = np.nan
        else:
             rho_df.loc[nu1, nu2] = np.nan


# Muestra la tabla de correlaciones
print("Tabla de correlaciones (rho_xy):")
rho_df

# prompt: export to latex table rho_df

print(rho_df.to_latex(float_format="%.4f"))

from scipy.interpolate import make_interp_spline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

nu1_vals = np.array([0.10, 0.25, 0.50, 1.0, 2.0, 3.0, 5.0, 10.0, 15.0, 20.0, 25.0])
nu2_vals = [0.5, 1.0, 2.0, 3.0, 5.0, 10.0, 20.0]

data_corr = np.array([
    [0.1556, 0.1866, 0.2067, 0.2141, 0.2186, 0.2168, 0.2071],
    [0.2307, 0.2757, 0.3088, 0.3213, 0.3296, 0.3290, 0.3161],
    [0.2948, 0.3554, 0.4022, 0.4208, 0.4345, 0.4372, 0.4237],
    [0.3554, 0.4339, 0.4979, 0.5249, 0.5468, 0.5567, 0.5462],
    [0.4022, 0.4979, 0.5800, 0.6167, 0.6493, 0.6702, 0.6675],
    [0.4208, 0.5249, 0.6167, 0.6591, 0.6981, 0.7265, 0.7301],
    [0.4345, 0.5468, 0.6493, 0.6981, 0.7449, 0.7827, 0.7949],
    [0.4372, 0.5567, 0.6702, 0.7265, 0.7827, 0.8323, 0.8553],
    [0.4312, 0.5529, 0.6713, 0.7314, 0.7927, 0.8486, 0.8774],
    [0.4237, 0.5462, 0.6675, 0.7301, 0.7949, 0.8553, 0.8880],
    [0.4160, 0.5385, 0.6618, 0.7263, 0.7938, 0.8579, 0.8937],
])

# Graficar curvas suavizadas
plt.figure(figsize=(10, 6))
nu1_dense = np.linspace(nu1_vals.min(), nu1_vals.max(), 300)

for i, nu2 in enumerate(nu2_vals):
    y_vals = data_corr[:, i]
    spline = make_interp_spline(nu1_vals, y_vals, k=3)
    y_smooth = spline(nu1_dense)
    plt.plot(nu1_dense, y_smooth, label=f"{nu2}", linewidth=2)

plt.xlabel(r"$\nu_1$")
plt.ylabel(r"$\rho_{X_1X_2}$")
plt.title(r"Correlation $\rho_{X_1X_2}$ as a function of $\nu_1$ for different $\nu_2$")
plt.axhline(0, color='black', linewidth=0.8)
plt.legend(title=r"$\nu_2$", loc="center left", bbox_to_anchor=(1, 0.5), fontsize=8, title_fontsize=9)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


def base_metropolis_hastings_ibtl(nu1, nu2, xi, num_samples=50, burn_in=500, proposal_std=2):
    # Initialize arrays to store samples
    samples = []
    accepted = 0
    # Initial values for X and Y (starting point)
    x, y = np.random.rand() * 2, np.random.rand() * 2  # Start with positive values

    for _ in range(num_samples + burn_in):
        # Propose new values for X and Y
        x_proposal = np.abs(x + np.random.normal(0, proposal_std))
        y_proposal = np.abs(y + np.random.normal(0, proposal_std))

        # Calculate the acceptance ratio
        current_pdf = pdf_ibtl(x, y, nu1, nu2, xi)
        proposed_pdf = pdf_ibtl(x_proposal, y_proposal, nu1, nu2, xi)
        acceptance_ratio = min(1, proposed_pdf / current_pdf) if current_pdf > 0 else 1

        # Accept or reject the proposal
        if np.random.rand() < acceptance_ratio:
            x, y = x_proposal, y_proposal
            accepted += 1
        # Collect samples after burn-in period
        if _ >= burn_in:
            samples.append((x, y))
    acceptance_rate = accepted / (num_samples + burn_in)
    return np.array(samples), acceptance_rate

def metropolis_hastings_ibtl(nu1, nu2, xi, num_samples=50, burn_in=500, proposal_std=0.5):
  sample, acceptance_rate = base_metropolis_hastings_ibtl(nu1=nu1, nu2=nu2, xi=xi,
                                                  num_samples=num_samples, burn_in=burn_in,
                                                  proposal_std=proposal_std)
  iterartion = 0
  while acceptance_rate > 0.4:
    sample, acceptance_rate = base_metropolis_hastings_ibtl(nu1=nu1, nu2=nu2, xi=xi,
                                                  num_samples=num_samples, burn_in=burn_in,
                                                  proposal_std=proposal_std)
    iterartion += 1
    if iterartion > 20:
      proposal_std += 0.1
  print(f"Tasa de aceptación: {acceptance_rate:.2f}")
  df_sample=pd.DataFrame(sample, columns = ['x', 'y'])
  return sample

from scipy.special import digamma
import numpy as np
import numpy as np
from scipy.special import gammaln

def log_likelihood_ibvl(nu1, nu2, xi, x, y):
    """
    Log-verosimilitud para la distribución bivariada invertida Topp-Leone.

    Parámetros
    ----------
    nu1 : float
        Parámetro de forma ν1 (> 0).
    nu2 : float
        Parámetro de forma ν2 (> 0).
    xi : float
        Parámetro de escala ξ (> 0).
    x, y : array_like
        Muestras de las variables aleatorias (todas > 0).

    Retorna
    -------
    float
        Valor de la log-verosimilitud.
    """
    x = np.asarray(x, dtype=float)
    y = np.asarray(y, dtype=float)
    n = len(x)
    z = x + y
    ratio = z / xi

    term1 = n * (np.log(2) + gammaln(nu1 + nu2 + 1) - gammaln(nu1) - gammaln(nu2) - (nu1 + nu2) * np.log(xi))
    term2 = (nu1 - 1) * np.sum(np.log(x))
    term3 = (nu2 - 1) * np.sum(np.log(y))
    term4 = - (2 * (nu1 + nu2) + 1) * np.sum(np.log(1 + ratio))
    term5 = (nu1 + nu2 - 1) * np.sum(np.log(2 + ratio))

    return term1 + term2 + term3 + term4 + term5


def derivative_likelihood_nu1(nu1, nu2, xi, x_samples, y_samples):
    n = len(x_samples)
    term1 = n * (digamma(nu1 + nu2 + 1) - digamma(nu1) - np.log(xi))
    term2 = np.sum(np.log(x_samples))
    term3 = -2 * np.sum(np.log(1 + (x_samples + y_samples) / xi))
    term4 = np.sum(np.log(2 + (x_samples + y_samples) / xi))

    return term1 + term2 + term3 + term4

def derivative_likelihood_nu2(nu1, nu2, xi, x_samples, y_samples):
    n = len(x_samples)
    term1 = n * (digamma(nu1 + nu2 + 1) - digamma(nu2) - np.log(xi))
    term2 = np.sum(np.log(y_samples))
    term3 = -2 * np.sum(np.log(1 + (x_samples + y_samples) / xi))
    term4 = np.sum(np.log(2 + (x_samples + y_samples) / xi))

    return term1 + term2 + term3 + term4

def derivative_likelihood_xi(nu1, nu2, xi, x_samples, y_samples):
    """
    Computes the derivative of the log-likelihood function with respect to xi.

    Parameters:
    - nu1 (float): First shape parameter.
    - nu2 (float): Second shape parameter.
    - xi (float): Scale parameter.
    - x_samples (np.ndarray): Samples of the first random variable.
    - y_samples (np.ndarray): Samples of the second random variable.

    Returns:
    - float: Value of the derivative with respect to xi.
    """
    n = len(x_samples)
    term1 = n * (nu1 + nu2) / xi
    term2 = (2 * (nu1 + nu2) + 1) / xi
    term3 =  np.sum(((x_samples + y_samples) / xi) / (1 + (x_samples + y_samples) / xi))
    term4 = ((nu1 + nu2 - 1) / xi) * np.sum(((x_samples + y_samples) / xi) / ((2 + (x_samples + y_samples) / xi)))

    return term1 + term2 + term3 -  term4

from scipy.optimize import root

# Función del sistema para pasar a root
def system_to_solve(params, x_samples, y_samples):
    nu1, nu2, xi = params
    if nu1 <= 0 or nu2 <= 0 or xi <= 0:
        return [1e6, 1e6, 1e6]  # Penalización para valores fuera del dominio

    dnu1 = derivative_likelihood_nu1(nu1, nu2, xi, x_samples, y_samples)
    dnu2 = derivative_likelihood_nu2(nu1, nu2, xi, x_samples, y_samples)
    dxi = derivative_likelihood_xi(nu1, nu2, xi, x_samples, y_samples)

    return [dnu1, dnu2, dxi]

# Función principal para obtener los MLEs
def estimate_mle(x_samples, y_samples, tol=1e-8, init_guess=(1.0, 1.0, 1.0), maxfev = 1000):
    sol = root(system_to_solve,
               args=(x_samples, y_samples),
               x0=init_guess,
               method='hybr',
               tol=tol,
               options={'maxfev': maxfev})
    if sol.success:
        return sol.x  # [nu1_hat, nu2_hat, xi_hat]
    else:
        raise RuntimeError("MLE root finding did not converge: " + sol.message)

nu1 = 3.0
nu2= 1.0
xi = 0.5
samples = metropolis_hastings_ibtl(num_samples=1000, nu1=nu1, nu2=nu2, xi=xi)
x_samples = samples[:,0]
y_samples = samples[:,1]
# Calcular estimaciones
nu1_hat, nu2_hat, xi_hat = estimate_mle(x_samples, y_samples, init_guess=(2.5, 0.8, 0.3))
print(f"MLEs: nu1 = {nu1_hat:.4f}, nu2 = {nu2_hat:.4f}, xi = {xi_hat:.4f}")

from scipy.optimize import minimize

def estimate_mle_ibvl_minimize(x_samples, y_samples, init_guess=(1.0, 1.0, 1.0)):
    """
    Estima los parámetros de la IBTL usando minimize (con norma del score).

    Retorna:
    np.ndarray: [nu1_hat, nu2_hat, xi_hat]
    """
    def objective(params):
        nu1, nu2, xi = params
        if nu1 <= 0 or nu2 <= 0 or xi <= 0:
            return 1e6
        score = system_to_solve(params, x_samples, y_samples)
        return np.sum(np.square(score))  # ||score||^2

    bounds = [(1e-6, None), (1e-6, None), (1e-6, None)]

    result = minimize(objective, x0=init_guess, bounds=bounds, method='L-BFGS-B')
    if result.success:
        return result.x
    else:
        raise RuntimeError(f"Minimize MLE did not converge: {result.message}")

nu1 = 3.0
nu2= 1.0
xi = 1
samples = metropolis_hastings_ibtl(num_samples=1000, nu1=nu1, nu2=nu2, xi=xi)
x_samples = samples[:,0]
y_samples = samples[:,1]
# Calcular estimaciones
mle_min = estimate_mle_ibvl_minimize(x_samples, y_samples, init_guess=(2.0, 1.0, 1.0))
print(f"MLE (minimize): ν₁ = {mle_min[0]:.4f}, ν₂ = {mle_min[1]:.4f}, ξ = {mle_min[2]:.4f}")